---
phase: 03-mobile-app-voice
plan: 05
type: execute
wave: 2
depends_on: ["03-02", "03-03"]
files_modified:
  - mobile-app/src/hooks/useSpeechRecognition.ts
  - mobile-app/src/components/RecordButton.tsx
  - mobile-app/src/components/WaveformVisualizer.tsx
  - mobile-app/src/components/RecordingTimer.tsx
autonomous: true

must_haves:
  truths:
    - "User taps button to start recording"
    - "User taps button again to stop recording"
    - "Animated waveform shown during recording"
    - "Timer shows recording duration"
    - "Live text appears as user speaks"
  artifacts:
    - path: "mobile-app/src/hooks/useSpeechRecognition.ts"
      provides: "Speech recognition state management"
      exports: ["useSpeechRecognition"]
    - path: "mobile-app/src/components/RecordButton.tsx"
      provides: "Tap-to-record button with states"
      min_lines: 30
    - path: "mobile-app/src/components/WaveformVisualizer.tsx"
      provides: "Animated waveform during recording"
      min_lines: 20
  key_links:
    - from: "mobile-app/src/hooks/useSpeechRecognition.ts"
      to: "mobile-app/src/services/speech.ts"
      via: "import { startListening, stopListening }"
      pattern: "startListening"
    - from: "mobile-app/src/components/RecordButton.tsx"
      to: "mobile-app/src/hooks/useSpeechRecognition.ts"
      via: "useSpeechRecognition()"
      pattern: "useSpeechRecognition"
---

<objective>
Create voice recording UI components and hook for capturing speech with visual feedback.

Purpose: Enable user to record speech with tap-to-start/stop, see animated waveform, timer, and live transcription.
Output: RecordButton, WaveformVisualizer, RecordingTimer components with useSpeechRecognition hook.
</objective>

<execution_context>
@C:\Users\luish\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\luish\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/phases/03-mobile-app-voice/03-CONTEXT.md
@.planning/phases/03-mobile-app-voice/03-RESEARCH.md
@mobile-app/src/types/index.ts
@mobile-app/src/services/speech.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create useSpeechRecognition hook</name>
  <files>
    mobile-app/src/hooks/useSpeechRecognition.ts
  </files>
  <action>
    Create useSpeechRecognition hook that manages recording state and speech capture.
    Per user decision:
    - Tap to start, tap to stop (two explicit taps)
    - Live streaming text as speech is recognized

    ```typescript
    import { useState, useEffect, useCallback, useRef } from 'react';
    import type { RecordingState, SpeechError } from '../types';
    import {
      requestSpeechPermission,
      checkSpeechPermission,
      isSpeechAvailable,
      setupSpeechListeners,
      startListening,
      stopListening,
      cleanupSpeechListeners,
    } from '../services/speech';

    interface UseSpeechRecognitionResult {
      state: RecordingState;
      liveText: string;
      finalText: string;
      error: SpeechError | null;
      recordingDuration: number;  // in seconds
      startRecording: () => Promise<void>;
      stopRecording: () => Promise<void>;
      setFinalText: (text: string) => void;
      clearError: () => void;
      resetToIdle: () => void;
      hasPermission: boolean;
      isAvailable: boolean;
    }

    /**
     * Hook for managing speech recognition with recording states.
     * Per user decision:
     * - Tap to start recording, tap to stop (two explicit taps)
     * - Live streaming text as speech is recognized (word-by-word updates)
     * - After stop: transition to 'editing' state
     */
    export function useSpeechRecognition(): UseSpeechRecognitionResult {
      const [state, setState] = useState<RecordingState>('idle');
      const [liveText, setLiveText] = useState('');
      const [finalText, setFinalText] = useState('');
      const [error, setError] = useState<SpeechError | null>(null);
      const [recordingDuration, setRecordingDuration] = useState(0);
      const [hasPermission, setHasPermission] = useState(false);
      const [isAvailable, setIsAvailable] = useState(true);

      const timerRef = useRef<ReturnType<typeof setInterval> | null>(null);
      const startTimeRef = useRef<number>(0);

      // Check permission and availability on mount
      useEffect(() => {
        const checkSetup = async () => {
          const available = await isSpeechAvailable();
          setIsAvailable(available);

          const permitted = await checkSpeechPermission();
          setHasPermission(permitted);
        };
        checkSetup();
      }, []);

      // Setup speech listeners
      useEffect(() => {
        setupSpeechListeners(
          // onPartialResults - live streaming text
          (text) => {
            setLiveText(text);
          },
          // onError - speech recognition error
          (speechError) => {
            setError(speechError);
            setState('idle');
            stopTimer();
          },
          // onListeningState
          (isListening) => {
            if (!isListening && state === 'recording') {
              // Speech recognition stopped unexpectedly (timeout)
              setFinalText(liveText);
              setState('editing');
              stopTimer();
            }
          }
        );

        return () => {
          cleanupSpeechListeners();
          stopTimer();
        };
      }, [liveText, state]);

      // Timer management
      const startTimer = useCallback(() => {
        startTimeRef.current = Date.now();
        setRecordingDuration(0);
        timerRef.current = setInterval(() => {
          const elapsed = Math.floor((Date.now() - startTimeRef.current) / 1000);
          setRecordingDuration(elapsed);
        }, 1000);
      }, []);

      const stopTimer = useCallback(() => {
        if (timerRef.current) {
          clearInterval(timerRef.current);
          timerRef.current = null;
        }
      }, []);

      // Start recording
      const startRecording = useCallback(async () => {
        try {
          // Request permission if not granted
          if (!hasPermission) {
            const granted = await requestSpeechPermission();
            setHasPermission(granted);
            if (!granted) {
              setError({ code: 9, message: 'Permiso de microfono denegado.' });
              return;
            }
          }

          setError(null);
          setLiveText('');
          setState('recording');
          startTimer();

          await startListening();
        } catch (err: any) {
          setError({ code: -1, message: err.message || 'Error al iniciar grabacion' });
          setState('idle');
          stopTimer();
        }
      }, [hasPermission, startTimer, stopTimer]);

      // Stop recording
      const stopRecording = useCallback(async () => {
        try {
          await stopListening();
          stopTimer();
          setFinalText(liveText);
          setState('editing');
        } catch (err: any) {
          setError({ code: -1, message: err.message || 'Error al detener grabacion' });
        }
      }, [liveText, stopTimer]);

      // Clear error
      const clearError = useCallback(() => {
        setError(null);
      }, []);

      // Reset to idle state
      const resetToIdle = useCallback(() => {
        setState('idle');
        setLiveText('');
        setFinalText('');
        setError(null);
        setRecordingDuration(0);
      }, []);

      return {
        state,
        liveText,
        finalText,
        error,
        recordingDuration,
        startRecording,
        stopRecording,
        setFinalText,
        clearError,
        resetToIdle,
        hasPermission,
        isAvailable,
      };
    }
    ```
  </action>
  <verify>
    File exists: `cat mobile-app/src/hooks/useSpeechRecognition.ts | head -40`
    Manages recording states: grep for "RecordingState"
    Uses speech service: grep for "import.*from.*speech"
  </verify>
  <done>
    useSpeechRecognition hook manages idle/recording/editing states.
    Captures live text via partial results.
    Tracks recording duration with timer.
    Handles permissions and errors.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create RecordButton and RecordingTimer components</name>
  <files>
    mobile-app/src/components/RecordButton.tsx
    mobile-app/src/components/RecordingTimer.tsx
  </files>
  <action>
    Create RecordButton for tap-to-start/stop and RecordingTimer for duration display.
    Per user decision: tap to start, tap to stop (two explicit taps).

    1. Create src/components/RecordingTimer.tsx:
    ```tsx
    interface RecordingTimerProps {
      seconds: number;
    }

    /**
     * Recording duration timer display.
     * Per user decision: timer shown during recording.
     */
    export function RecordingTimer({ seconds }: RecordingTimerProps) {
      const minutes = Math.floor(seconds / 60);
      const remainingSeconds = seconds % 60;
      const formatted = `${minutes}:${remainingSeconds.toString().padStart(2, '0')}`;

      return (
        <div className="text-2xl font-mono text-gray-700 tabular-nums">
          {formatted}
        </div>
      );
    }
    ```

    2. Create src/components/RecordButton.tsx:
    ```tsx
    import type { RecordingState } from '../types';

    interface RecordButtonProps {
      state: RecordingState;
      onStart: () => void;
      onStop: () => void;
      disabled?: boolean;
    }

    /**
     * Tap-to-record button with visual states.
     * Per user decision:
     * - Tap to start recording, tap to stop (two explicit taps)
     * - Visual feedback for recording state
     */
    export function RecordButton({
      state,
      onStart,
      onStop,
      disabled = false,
    }: RecordButtonProps) {
      const isRecording = state === 'recording';

      const handlePress = () => {
        if (disabled) return;
        if (isRecording) {
          onStop();
        } else {
          onStart();
        }
      };

      // Don't show button in editing state
      if (state === 'editing') {
        return null;
      }

      return (
        <button
          onClick={handlePress}
          disabled={disabled}
          className={`
            w-20 h-20 rounded-full
            flex items-center justify-center
            transition-all duration-200
            shadow-lg active:shadow-md active:scale-95
            ${disabled ? 'opacity-50 cursor-not-allowed' : 'cursor-pointer'}
            ${
              isRecording
                ? 'bg-red-500 hover:bg-red-600'
                : 'bg-blue-500 hover:bg-blue-600'
            }
          `}
          aria-label={isRecording ? 'Detener grabacion' : 'Iniciar grabacion'}
        >
          {isRecording ? (
            // Stop icon (square)
            <div className="w-6 h-6 bg-white rounded-sm" />
          ) : (
            // Microphone icon
            <svg
              className="w-8 h-8 text-white"
              fill="currentColor"
              viewBox="0 0 24 24"
            >
              <path d="M12 14c1.66 0 3-1.34 3-3V5c0-1.66-1.34-3-3-3S9 3.34 9 5v6c0 1.66 1.34 3 3 3z" />
              <path d="M17 11c0 2.76-2.24 5-5 5s-5-2.24-5-5H5c0 3.53 2.61 6.43 6 6.92V21h2v-3.08c3.39-.49 6-3.39 6-6.92h-2z" />
            </svg>
          )}
        </button>
      );
    }
    ```
  </action>
  <verify>
    Files exist: `ls mobile-app/src/components/Record*.tsx`
    RecordButton has tap logic: grep for "handlePress"
    RecordingTimer formats time: grep for "padStart"
  </verify>
  <done>
    RecordButton toggles between start/stop on tap.
    Shows microphone icon when idle, stop icon when recording.
    RecordingTimer displays duration in MM:SS format.
    Both components hidden in editing state.
  </done>
</task>

<task type="auto">
  <name>Task 3: Create WaveformVisualizer component</name>
  <files>
    mobile-app/src/components/WaveformVisualizer.tsx
  </files>
  <action>
    Create WaveformVisualizer with CSS-based animation.
    Per research open question: react-audio-visualize may not integrate with SpeechRecognition.
    Using CSS animation approach as fallback per research recommendation.
    Per user decision: animated waveform during recording.

    ```tsx
    interface WaveformVisualizerProps {
      isRecording: boolean;
    }

    /**
     * Animated waveform visualization during recording.
     * Per user decision: animated waveform + timer during recording.
     * Uses CSS animation since SpeechRecognition doesn't expose audio stream.
     */
    export function WaveformVisualizer({ isRecording }: WaveformVisualizerProps) {
      if (!isRecording) {
        return null;
      }

      // Create 7 bars with staggered animation
      const bars = [0, 1, 2, 3, 4, 5, 6];

      return (
        <div className="flex items-center justify-center gap-1 h-16">
          {bars.map((index) => (
            <div
              key={index}
              className="w-2 bg-blue-500 rounded-full animate-waveform"
              style={{
                animationDelay: `${index * 0.1}s`,
                height: '100%',
              }}
            />
          ))}
          <style>{`
            @keyframes waveform {
              0%, 100% {
                transform: scaleY(0.3);
              }
              50% {
                transform: scaleY(1);
              }
            }
            .animate-waveform {
              animation: waveform 0.8s ease-in-out infinite;
              transform-origin: center;
            }
          `}</style>
        </div>
      );
    }
    ```
  </action>
  <verify>
    File exists: `cat mobile-app/src/components/WaveformVisualizer.tsx`
    Has animation: grep for "animate-waveform"
    Hidden when not recording: grep for "if.*!isRecording"
  </verify>
  <done>
    WaveformVisualizer shows animated bars during recording.
    Uses CSS animation (fallback per research).
    Hidden when not recording.
    7 bars with staggered animation for visual effect.
  </done>
</task>

</tasks>

<verification>
1. `ls mobile-app/src/hooks/useSpeechRecognition.ts` exists
2. `ls mobile-app/src/components/RecordButton.tsx` exists
3. `ls mobile-app/src/components/WaveformVisualizer.tsx` exists
4. `cd mobile-app && npx tsc --noEmit` passes
</verification>

<success_criteria>
- useSpeechRecognition manages idle/recording/editing states
- useSpeechRecognition captures live text during recording
- RecordButton toggles between start/stop on tap
- RecordButton disabled when specified (offline, no devices)
- WaveformVisualizer shows animated bars during recording
- RecordingTimer displays duration in MM:SS format
- TypeScript compiles without errors
</success_criteria>

<output>
After completion, create `.planning/phases/03-mobile-app-voice/03-05-SUMMARY.md`
</output>
